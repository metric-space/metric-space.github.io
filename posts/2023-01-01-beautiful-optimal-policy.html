<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Metric Space -  Reinforcement Learning - Beautiful optimal policy</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../js/highlight/styles/nord.css">
        <script src="../js/highlight/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Metric-Space</a>
            </div>
            <div id="navigation">
                <a href="../index.html">Back</a>
            </div>
        </div>

        <div id="content">
          <div>
            <h3>The problem</h3>
            <p>(This problem is Ex. 4.8 in the book <b><a href="http://www.incompleteideas.net/book/the-book-2nd.html" target="_blank"> Reinforcement Learning (by Richard S. Sutton and Andrew G. Barton))</a></b></p>
            <p>You have a certain amount &lt; 100 dollars with you. On multiple coin tosses, for a single coin toss you can stake an amount to reach the goal of 100 dollars. Heads you win the amount you stake and tails you loose the amount you stake. Game ends when you hit 0 dollars. <b> The probability of a heads is 0.4 </b>.</p><p>You are given an optimal plan of action (graph below) for any start value to reach a 100 and are asked</p>
            <img src="../images/optimal_policy.png"></img>
            <h3>Why does the optimal policy have a curious form?</h3>
          </div>
          <hr/>
          <div>
            <h4> Notes: </h4>
            <ol>
              <li>
                <div>
                  <p> In the literature preceding the actual question, the actions are limited to staking amounts in the range of <b>0</b> to <b>min(s, 100-s)</b> where <b>s</b> is your current amount. (<b> THIS IS IMPORTANT </b>)</p>
                  <p> For example, if you have 70 dollars on hand, theoretically you can stake any amount from 0 dollars to 30 dollars (as per the above rule) </p>
                </div>
              </li>
              <li>
                <figure>
                  <img src="../images/value_estimation.png"></img>
                  <figcaption> A graph depicting the odds of getting to the goal from your current state</figcaption>
                </figure>
              </li>
              <li>
                <div>
                  <p>Definition of a <b>Policy</b>:<p>
                  <p>A  function that given your current state gives out an action to take you to the next state (this can be deterministic or stochastic. For this problem, this is deterministic in nature) </p>
                </div>
              </li>
            </ol>
          </div>
            <hr/>
            <h3>A look at how bad 0.4 chance of heads is in terms of winning</h3>
            <p> To get a sense of how bad things are in terms of winning, we plot the number of wins per thousand games for various starting amounts for three different types of coin toss distributions </p>
            <p>The strategy used here is one where we go all in, all the time. I choose to take this as my baseline in the constrast against the optimal policy (shown later on)</p>
            <image src="../images/baseline.png"></image>
            <hr/>
            <h3> Some thoughts on a possible optimal solution </h3>
            <ul>
              <li> The optimal solution takes into consideration that loss is inevitable, and takes advantage of this. (<b>something that the baseline cannot do</b>)</li>
              <li> The strategy on heads should be to try to get to 100 with minimal steps because you're likely to lose because the odds of you winning in a sequence is stacked against you </li>
              <li> Again since you're more likely to lose, you want to make sure that your losses are funneled away from a Knock-out to positions that might contribute to a potential win on the next toss.</li>
              <li> From the above you'd want to concentrate your sweet spots to a few places where you drive in a winning shot in one go OR if not that be a jump spot to a winning spot </li>
              <li> You want to make sure there is enough distance between these sweet/jump spots. And the reasoning for that follows below</li>
              <li> You'd want to make sure that every state that cannot possibly reach the goal in one sweep somehow gets closer to the sweet/jump spots on win or lose </li>
            </ul>
            <h4> How the above points translate to the optimal policy in question </h4>
            <p> Refer to <a href="#footnotes">[1]</a> for actual points</p>
            <quote>
               <i>you'd want to concentrate your sweet spots to a few places where you drive in a winning shot in one go OR if not that be a jump spot to a winning spot</i>
            </quote>
            <br/>
            <ul>
              <li> states 50 , 75 and states &ge; 88 are sweet spots </li>
              <li> states 50 and 75 are special in that they contribute to the strategy of other states. Coincidentally a failure on 75 leads to us ending up on 50. A sweet spot assisting another sweet spot!</li>
              <li> 25 is a jump spot as it serves as a spot to shuttle to the sweet spot 50. I call attention to 25 as a jump spot as it serves this purpose to different states on win and to different states on loss </li>
            </ul>
            <quote>
              <i> You'd want to make sure that every state that cannot possibly reach the goal in one sweep somehow gets closer to the sweet/jump spots on win or lose </i>
            </quote>
            <ul>
              <li> If you look at states &ge; 13 and &le; 24, on win they reach the jump spot 25. On loss, they are knocked back to a a dreary point but atleast the game hasn't ended</li>
              <li> If you look at states &ge; 26 and &le; 37, on loss they reach the jump spot 25, and on winning they are closer to the sweet spot 50</li>
              <li> If you look at states &ge; 63 and &le; 74, on win they reach the sweet spot 75. On loss they are knocked back a bit but are lead closer to sweet spot 50 </li>
              <li> If you look at states &ge; 51 and &le; 62, on loss they are on the sweet spot 50
              <li> If you look at states &ge; 76 and &le; 87, on loss they reach the sweet spot 75, and on winning they inch closer to the goal </li>
            </ul>
            <p> The rest of the states i.e &ge; 1 and &le; 12 go all in and get closer to 25 (can't do significantly much here in the scheme of this policy); &gt; 88 and &le; 99 are already within the range of the goal in one shot</p>
            <quote>
               <i> You want to make sure there is enough distance between these sweet/jump spots</i>
            </quote>
            <p> The choice of 25, 50 and 75 with the spacing that comes along with those, along with the goal of reaching the <b>exact amount of 100</b> and the funneling of the other states on wins/loses is responsible for the curious form <p>
            <p><b>If the actions were permissive of over-reaching the goal i.e getting an amount of more than 100, the policy would look different</b></p>
          </div>
          <hr/>
          <h3> Closing thoughts </h3>
          <p> Here's a graph that shows us how the optimal policyfares against the baseline which serves as evidence that it is a good solution. </p>
          <figure>
            <image src="../images/baseline_vs_optimal.png"></image>
          </figure>

          <p> We've established why the optimal policy has a curious form. One so symmetric and beautiful. The thing of beauty is how the policy manages loss to provide wins. </p>
        </div>
        <hr/>
        <div id="footnotes">
          <h4>Footnotes</h4>
          <p>[1] Optimum policy state-action (x,y) pairs</p>
          <pre>
              [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12),
              (13, 12), (14, 11), (15, 10), (16, 9), (17, 8), (18, 7), (19, 6), (20, 5), (21, 4), (22, 3), (23, 2), (24, 1),
              (25, 25),
              (26, 1), (27, 2), (28, 3), (29, 4), (30, 5), (31, 6), (32, 7), (33, 8), (34, 9), (35, 10), (36, 11), (37, 12),
              (38, 12), (39, 11), (40, 10), (41, 9), (42, 8), (43, 7), (44, 6), (45, 5), (46, 4), (47, 3), (48, 2), (49, 1),
              (50, 50),
              (51, 1), (52, 2), (53, 3), (54, 4), (55, 5), (56, 6), (57, 7), (58, 8), (59, 9), (60, 10), (61, 11), (62, 12),
              (63, 12), (64, 11), (65, 10), (66, 9), (67, 8), (68, 7), (69, 6), (70, 5), (71, 4), (72, 3), (73, 2), (74, 1),
              (75, 25),
              (76, 1), (77, 2), (78, 3), (79, 4), (80, 5), (81, 6), (82, 7), (83, 8), (84, 9), (85, 10), (86, 11), (87, 12),
              (88, 12), (89, 11), (90, 10), (91, 9), (92, 8), (93, 7), (94, 6), (95, 5), (96, 4), (97, 3), (98, 2), (99, 1)]
          </pre>
        </div>
        <hr/>
        <h4> Acknowledgements </h4>
        <p> Special thanks to Spencer for intensive feedback on structure and content of this blog post</p>
        <div id="footer">
            <a href="https://www.brainyquote.com/authors/alfred_north_whitehead" target="_blank">Quotes by an okay chap</a>
        </div>
    </body>
</html>
