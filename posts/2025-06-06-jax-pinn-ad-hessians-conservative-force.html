<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Metric Space - Implementing Physics constraints in PINNs with JAX </title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../js/highlight/styles/nord.css">
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
		<link rel="stylesheet" href="../js/highlight/styles/nord.min.css">
        <script src="../js/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Metric-Space</a>
            </div>
            <div id="navigation">
                <a href="../index.html">Back</a>
            </div>
        </div>

        <div id="content">
            <div> 
        <h3 id="Intro"> Problem Intro</h3>
		<p> You have an untrained neural net that spits a gravity scalar potential \( \mathbb{U} \) in response to a 3D cartesian position </p>
		<p> The goal is to figure out gravitational acceleration from the gravity scalar potential </p>
		<p> Turns out the gravitational acceleration \( \mathbf{a}(\mathbf{x}) \) is obtained as the negative gradient of the potential: </p> 
		<p> \[ \mathbf{a}(\mathbf{x}) = -\nabla U(\mathbf{x}) \] </p>
		<br/>
		<p> At some point we want to train the neural net, so inevitably we'll get a dataset of 3d cartesian products mapped to their respective acceleration values </p>

		<p> But what does the loss function to train the NN look like? For this we take a look at the loss function described in this paper: <a href="https://hanspeterschaub.info/PapersPrivate/Martin2022d.pdf">Physics-informed neural networks for gravity field modeling of small bodies</a></p>

		<p> The loss in question is here: </p>
		<p> \[ \mathcal{L}(\theta) = \underbrace{\frac{1}{N_f} \sum_{i=1}^{N_f} \left\| \mathbf{a_i}_{\text{true}} - (- \nabla U(\mathbf{x}_{i})) \right\|^2}_{\text{acceleration error}} + \underbrace{\frac{1}{N_f} \sum_{i=1}^{N_f} \left\| \nabla^2 U(\mathbf{x}_i) \right\|^2}_{\text{Laplacian penalty}} + \underbrace{\frac{1}{N_f} \sum_{i=1}^{N_f} \left\| \nabla \times \nabla U(\mathbf{x}_i) \right\|^2}_{\text{curl penalty}} \] </p>
		<p> The first term probably makes sense given it is your basic MSE with the true acceleration values. As for the rest ... <p>

		<p> The paper's contribution is that since gravity is a conservative force, the loss function could be significantly enhanced by additional dynamic properties</p>

		<p> The scalar potential learned by the network must also obey these additional physics properties </p>

		<ol>
			<li> \( \nabla^2 U = 0 \) </li>
			<li> \( \nabla \times \mathbf{a} = 0 \) </li>
		</ol>

		<p> So the cool bit is that by including these terms, we ensure the model being learnt is more in line with the physics of things which in this case makes sure that force of gravity being modelled in inline with gravity being a conservative force and the physics that comes along with that fact  </p>

		<p> That's all good and dandy but how do we put this in code? </p>

		<p> We'll be looking at implementing this in <a href="https://docs.jax.dev/en/latest/index.html">Jax</a></p>

		<p> The MSE part is straightward but it's coding in the additional constraints that is not exactly straightforward as will be shown. <strong>This blog post is about figuring out how to encode the additional contraints related to acceleration field in Jax</strong> </p>

		<hr/>

		<h3> Brief tangent </h3>

		<p> <i> Hang on, why bother with a NN spitting out a scalar potential when it can spit 3 ripe values for acceleration </i> ? </p>

		<blockquote style="border:solid #5b6dcd 2px"> "Most obvious is the fact that the network is now trained with the knowledge that there is a relationship between the accelerations it produces via automatic differentiation and the more
			fundamental scalar potential. Second, because the network is learning a representation of the
			potential rather than the accelerations, all three acceleration vector components of the training
			data are now being used to constrain a single scalar function. This is a much more efficient
			regression for the network, learning a single potential rather than being forced to learn three
			separate acceleration features" 
		</blockquote>

		<p> <strong>Something about this claim (i.e 1 is more efficient than 3) should get your spidey sense tingling. I do have some thoughts on this as I think it is a weak claim and I promise to address this  in a future blog post</strong></p>

	   </div>

	   <hr/>

	   <div>
		   <h2 id="calculating"> Calculating the Laplacian of Scalar Potential and the Curl of the acceleration field</h2>

		   <p> <u>There are no jax primitives to calculate these directly </u> To get going, a refresher in Vector Calculus is order </p>

		   <h4>Gradient</h4>
		   <p> A vector valued function that takes in a scalar valued (differential) function (of several variables!) </p>
		   <p> A more precise way of expressing the domain and range of the function \( f \) and the gradient of the function  \( \nabla f \) is: </p>
		   <p> \[ f: \mathbb{R}^n \rightarrow \mathbb{R}, \quad \nabla f: \mathbb{R}^n \rightarrow \mathbb{R}^n \]</p>
		   <p> And now how the gradient is computed at a point \[ p = ( x, y, z) \]  (Note: we're using just 3 variables but that is not a constraint in general)</p>
		   <p> \[ \nabla f(p) = \frac{\partial f(p)}{\partial x}\mathbf{i} + \frac{\partial f(p)}{\partial y}\mathbf{j} + \frac{\partial f(p)}{\partial z}\mathbf{k} \] </p>

		   <br/>

		   <h4>Jacobian: Generalization of the Gradient </h4>
		   <p> While the input to a gradient is a scalar function, and some point it makes to ask if there is an equivalent if the input is a vector valued function <p>
		   <p> For a function \( \mathbf{F} : \mathbb{R}^n \rightarrow \mathbb{R}^m \), the Jacobian is an \( m \times n \) matrix whose \( (i, j) \)-th entry is \( \partial F_i / \partial x_j \) </p>
		   <p> To make things more concrete, let's say we have a function \( \mathbf{F} : \mathbb{R}^3 \rightarrow \mathbb{R}^3 \) and takes in \( \mathbf{p} = ( x, y, z) \in \mathbb{R}^{3}  \) as input and produces \( (f_{0}(\mathbf{p}), f_{1}(\mathbf{p}), f_{2}(\mathbf{p})) \in \mathbb{R}^{3} \) </p>
		   <p> The Jacobian is: </p>
		   <p> \[ J_{\mathbf{F}}(\mathbf{p}) = \begin{bmatrix} \frac{\partial f_0}{\partial x} & \frac{\partial f_0}{\partial y} & \frac{\partial f_0}{\partial z} \\ \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} & \frac{\partial f_1}{\partial z} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} & \frac{\partial f_2}{\partial z} \end{bmatrix} \] </p>

		   <br/>

		   <h4>Divergence</h4>
		   <p> While the gradient maps a scalar function to a vector field (a vector field pretty much a vector valued function [there's a bit more to this]) , and the Jacobian maps a vector function to a matrix field (pretty much a matrix valued function), the divergence is a scalar-valued function applied to a vector field. </p>
		   <p>Computationally that means vector in scalar out </p>
		   <p> For a vector field \( \mathbf{F} : \mathbb{R}^n \rightarrow \mathbb{R}^n \), the divergence is defined as the dot product of the del operator with the vector field: </p>
		   <p> \[ \nabla \cdot \mathbf{F} = \sum_{i=0}^{n-1} \frac{\partial F_i}{\partial x_i} \] </p>
		   <p> More concretely, if \( \mathbf{F}(x, y, z) = (F_0, F_1, F_2) \), then: </p>
		   <p> \[ \nabla \cdot \mathbf{F} = \frac{\partial F_0}{\partial x} + \frac{\partial F_1}{\partial y} + \frac{\partial F_2}{\partial z} \] </p>


		   <br/>

		   <h4> Curl </h4>
		   <p> Something in the same line as the divergence is that in that it operates on a vector field. <strong> Something interesting to note about this operator is doesn't generalize beyond 3D space</strong>. The mechanical reasoning for this is evident if you see how the curl of a vector field is calculated</p>
		   <p> For a vector field \( \mathbf{F}(x, y, z) = (F_0, F_1, F_2) \), the curl is defined as the cross product of the del operator with \( \mathbf{F} \): </p>
		   <p> \[ \nabla \times \mathbf{F} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ F_0 & F_1 & F_2 \end{vmatrix} \] </p>
		   <p> Expanded out, this becomes: </p>
		   <p> \[ \nabla \times \mathbf{F} = \left( \frac{\partial F_2}{\partial y} - \frac{\partial F_1}{\partial z}\right) \mathbf{i} + \left( \frac{\partial F_0}{\partial z} - \frac{\partial F_2}{\partial x}\right)\mathbf{j} + \left(\frac{\partial F_1}{\partial x} - \frac{\partial F_0}{\partial y} \right)\mathbf{k} \] </p>
		   <p> Curiously the result is a new vector field representing the axis and intensity of local rotation of the input vector field. </p>



		   <h4> Laplacian </h4>
		   <p> (Parroting the <a href="https://en.wikipedia.org/wiki/Laplace_operator">wikipedia article on the same</a>) The Laplacian is a differential  operator that can be thought of as the divergence of the gradient of a scalar valued function. </p>
		   <p> For a scalar-valued function \( f : \mathbb{R}^n \rightarrow \mathbb{R} \), the Laplacian is defined as:</p>
		   <p> \[ \Delta f = \nabla \cdot \nabla f = \nabla^2 f  = \sum_{i=0}^{n-1} \frac{\partial^2 f}{\partial x_i^2} \] </p>

		   <br/>

		   <h4> Hessian</h4>
		   <p> <u>The mother lode when it comes to our computation of everything we need (for) </u> and this will be made more evident shortly. </p>
		   <p> The Hessian is a square matrix of second-order partial derivatives of a scalar-valued function </p>
		   <p> To put it another way, the Hessian of a function \( \mathbf{f} \) is the Jacobian matrix of the gradient of the function \( \mathbf{f} \) that is: \( \mathbf{H}(f(\mathbf{x})) = \mathbf{J}( \nabla f(\mathbf{x})) \). </p>
		   <p> For a scalar function \( f : \mathbb{R}^n \rightarrow \mathbb{R} \), the Hessian matrix is: </p>
		   <p> \[ H_f(x) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{i,j=1}^{n} \] </p>
		   <p> So if \( f \) is a function of three variables i.e. \( f(x, y, z) \), then: </p>
		   <p> \[ H_f(x, y, z) = \begin{bmatrix} \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} & \frac{\partial^2 f}{\partial x \partial z} \\ \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} & \frac{\partial^2 f}{\partial y \partial z} \\ \frac{\partial^2 f}{\partial z \partial x} & \frac{\partial^2 f}{\partial z \partial y} & \frac{\partial^2 f}{\partial z^2} \end{bmatrix} \] </p>
	   </div>
	   <div>
		   <h3> Jax primitives to calculate these:  </h3>
		   <ul>
			   <li> <strong>Gradient</strong>:<br/> <code>jax.grad(f)</code> computes the gradient of a scalar valued function. This is <a href="https://docs.jax.dev/en/latest/_autosummary/jax.grad.html#jax.grad">well documented</a>. This machine learning 101, no questions here   </li> 
			   <li> <strong>Jacobian</strong>:<br/> <code>jax.jacobian(F)</code> computes the full Jacobian matrix of a vector-valued function. Again another <a href="https://docs.jax.dev/en/latest/_autosummary/jax.jacobian.html#jax.jacobian">well documented</a> function though not as known as calculating the gradient.</li>
			   <li> <strong>Divergence</strong>: <br/> <u>No direct Jax primitive</u> but can be computed as the trace of the Jacobian: <br/> <code>jax.jacobian(F)(x).trace()</code> when \( F : \mathbb{R}^n \rightarrow \mathbb{R}^n \) </li>
			   <li> <strong>Curl</strong>: <br/> <u>No direct Jax primitive</u> It must be manually computed using partial derivatives and the determinant form in 3D </li>
			   <li> <strong>Laplacian</strong>: <br/> <u>No direct Jax primitive</u> but can be computed as the trace of the Hessian: <br/>
				   <code>jax.hessian(f)(x).trace()</code> for \( f : \mathbb{R}^n \rightarrow \mathbb{R} \)
			   </li>
			   <li> <strong>Hessian</strong>: <br/> <code>jax.hessian(f)</code> computes the full matrix of second derivatives for a scalar function. The <a href="https://docs.jax.dev/en/latest/_autosummary/jax.hessian.html#jax.hessian">documentation exists here</a></li> </ul>
		   <br/>
		   <h3> Closer look at operators that have no direct Jax routes </h3>
		   <p> When we take a closer look at the <strong>laplacian</strong> and keep staring it for a bit, we see that it is the just the diagonal of the Hessian. </p>
		   <p>
			\[
			H_f(x, y, z) =
			\begin{bmatrix}
			  \colorbox{yellow}{$\displaystyle \frac{\partial^2 f}{\partial x^2}$} & \frac{\partial^2 f}{\partial x \partial y} & \frac{\partial^2 f}{\partial x \partial z} \\
			  \frac{\partial^2 f}{\partial y \partial x} & \colorbox{yellow}{$\displaystyle \frac{\partial^2 f}{\partial y^2}$} & \frac{\partial^2 f}{\partial y \partial z} \\
			  \frac{\partial^2 f}{\partial z \partial x} & \frac{\partial^2 f}{\partial z \partial y} & \colorbox{yellow}{$\displaystyle \frac{\partial^2 f}{\partial z^2}$}
			\end{bmatrix}
			\]
		  </p>
		  <p> So what? well given that we have no native jax operation for such, this makes our life easier so all we need to do to calculate the Laplacian or just the divergence of the scalar potential (for our loss function) is:</p>
		  <pre><code class="python">
import jax
import jax.numpy

scalar_potential = model(x)

hessian = jnp.hessian(scalar_potential)
# or you could calculate the hessian with the grad 
# i.e jnp.jacobian(jnp.grad(scalar_potential))

laplacian = jnp.trace(hessian)
		  </code></pre>

		  <p> What about the curl of the acceleration field? <i> For this particular case since we are allowed to start with the scalar potential, it's the hessian to the rescue again </i></p>
		  <p> What do I mean? Let's look at the formula for curl of acceleration </p>
		  <p> \[ \nabla \times \mathbf{F} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ A_x & A_y & A_z \end{vmatrix} \] </p>
		  <p> Remember \( A_x, A_y, A_z \) are the three base components of the acceleration which is nothing but </p>
		  <p> \[ A_x \mathbf{i} + A_y \mathbf{j} + A_z \mathbf{k} = \frac{\partial s}{\partial x} \mathbf{i} + \frac{\partial s}{\partial y} \mathbf{j} + \frac{\partial s}{\partial z} \mathbf{k}  \]</p>
		  <p> so: </p>
		  <p> \[ \nabla \times \nabla \mathbf{s} = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\  \frac{\partial s}{\partial x} & \frac{\partial s}{\partial y} & \frac{\partial s}{\partial z} \end{vmatrix} \] </p>
		  <p> \[ \nabla \times \nabla \mathbf{s} = \left( \frac{\partial^2 s}{\partial y \partial z} - \frac{\partial^2 s}{\partial z \partial y}\right) \mathbf{i} + \left( \frac{\partial^2 s}{\partial z \partial x} - \frac{\partial^2 s}{\partial x \partial z}\right)\mathbf{j} + \left(\frac{\partial^2 s}{\partial x \partial y} - \frac{\partial^2 s}{\partial y \partial x} \right)\mathbf{k} \] </p>

		  <p> You dear reader probably have seen enough, but please be so kind as to allow me to drive home the point </p>
		  <p>
			\[
			H_f(x, y, z) =
			\begin{bmatrix}
			  \frac{\partial^2 f}{\partial x^2} &  \colorbox{yellow}{$\displaystyle \frac{\partial^2 f}{\partial x \partial y} $} & \colorbox{pink}{$\displaystyle \frac{\partial^2 f}{\partial x \partial z} $} \\
			  \colorbox{yellow}{$\displaystyle \frac{\partial^2 f}{\partial y \partial x} $} &  \frac{\partial^2 f}{\partial y^2} & \colorbox{green}{$\displaystyle \frac{\partial^2 f}{\partial y \partial z} $} \\
			   \colorbox{pink}{$\displaystyle \frac{\partial^2 f}{\partial z \partial x} $} & \colorbox{green}{$\displaystyle \frac{\partial^2 f}{\partial z \partial y} $} & \frac{\partial^2 f}{\partial z^2}
			\end{bmatrix}
			\]
		  </p>
		  <p> <strong> Note: </strong> Shouldn't this always be equal to zero? <i> Turns out not always </i> but it should be equal to zero as a constraint when search the space of all potentials </p>
		  <pre><code class="python">
import jax
import jax.numpy

scalar_potential = model(x)

hessian = jnp.hessian(scalar_potential)
# or you could calculate the hessian with the grad 
# i.e jnp.jacobian(jnp.grad(scalar_potential))

curl_x = jnp.array([hessian[2,1] - hessian[1,2]])
curl_y = jnp.array([hessian[2,0] - hessian[0,2]])
curl_z = jnp.array([hessian[1,0] - hessian[0,1]])
curl_of_s = jnp.stack((curl_x, curl_y, curl_z))
		  </code></pre>




	   </div>

	   <p>So all of this is cool, but does it really work? Let's put this to the test on a analytical scalar potential field defined on a point mass</p>
	   <p> Which is \( U(\mathbf{r}) = \frac{1}{\|\mathbf{r}\|} = \frac{1}{\sqrt{x^2 + y^2 + z^2}} \) (ignoring the negative sign and any constants)</p>
	   <p> Calculating the divergence and curl of the acceleration field derived from this should give us close to 0 for pretty much all coordinates not equal to zero</p>
	   <pre><code class="python">
import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

f = lambda x: 1./jnp.linalg.norm(x) # you could multiply this with a high constant like 1000, it still be close to zero

acceleration_field = jax.grad(f)

def curl(vector_field):
    def f(x):
        hessian = jax.jacobian(vector_field)(x)
        curl_x = jnp.array([hessian[2,1] - hessian[1,2]])
        curl_y = jnp.array([hessian[2,0] - hessian[0,2]])
        curl_z = jnp.array([hessian[1,0] - hessian[0,1]])
        return jnp.stack([curl_x, curl_y, curl_z], axis=-1)
    return f


def divergence(vector_field):
    def f(x):
        return jnp.trace(jax.jacobian(vector_field)(x))
    return f


v_curl = jax.vmap(curl(acceleration_field))
v_divergence = jax.vmap(divergence(acceleration_field))

points = jnp.linspace(-2.0, 2.0, 10)
X,Y,Z = jnp.meshgrid(points, points, points)
X,Y,Z = X.reshape(-1), Y.reshape(-1), Z.reshape(-1)
coords = jnp.stack([X,Y,Z], axis=-1)

# we want to avoid anything too close to zero
filtered_points = coords[jnp.nonzero(jnp.linalg.norm(coords, axis=-1) > 0.1)]

curls = jnp.linalg.norm(v_curl(filtered_points), axis=-1)
divs = jnp.abs(v_divergence(filtered_points))

plt.figure(figsize=(10, 5))
plt.subplot(1,2,1)
plt.hist(curls, bins=30, color='skyblue')
plt.title("Curl Magnitude Distribution")
plt.xlabel("curl magnitude")
plt.ylabel("frequency")

plt.subplot(1, 2, 2)
plt.hist(divs, bins=30, color='salmon')
plt.title("Divergence Magnitude Distribution")
plt.xlabel("divergence magnitude")
plt.ylabel("frequency")

plt.tight_layout()
plt.savefig("curl_and_div.png")
	   </code></pre>
	   <figure>
		<image src="../images/curl_and_div.png"></image>
	  </figure>
	  <p> Seems like we have the magnitudes of the curl and diverge of the the acceleration field associated with our analytical potential function,  very very close to zero providing strong evidence that our computation is right </p>

	<h3>Conclusion</h3>
	<p> Given that we now know how to compute the curl and divergence in Jax, encoding the constraints for the conservative force should be a lot more straightforward </p>
	</div>

	<div id="footer">
            <a href="https://www.brainyquote.com/authors/alfred_north_whitehead" target="_blank">Quotes by an okay chap</a>
        </div>
    </body>
</html>
